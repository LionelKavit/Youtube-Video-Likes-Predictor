{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-07T23:20:52.886261Z",
     "iopub.status.busy": "2024-05-07T23:20:52.885695Z",
     "iopub.status.idle": "2024-05-07T23:20:52.891293Z",
     "shell.execute_reply": "2024-05-07T23:20:52.889943Z",
     "shell.execute_reply.started": "2024-05-07T23:20:52.886221Z"
    },
    "papermill": {
     "duration": 0.050719,
     "end_time": "2022-02-09T18:54:39.289296",
     "exception": false,
     "start_time": "2022-02-09T18:54:39.238577",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T23:20:52.897574Z",
     "iopub.status.busy": "2024-05-07T23:20:52.896675Z",
     "iopub.status.idle": "2024-05-07T23:20:52.903976Z",
     "shell.execute_reply": "2024-05-07T23:20:52.903005Z",
     "shell.execute_reply.started": "2024-05-07T23:20:52.897545Z"
    },
    "papermill": {
     "duration": 12.845949,
     "end_time": "2022-02-09T18:54:52.160645",
     "exception": false,
     "start_time": "2022-02-09T18:54:39.314696",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import regex as re\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs,string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from sklearn import model_selection\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T23:20:52.906363Z",
     "iopub.status.busy": "2024-05-07T23:20:52.906031Z",
     "iopub.status.idle": "2024-05-07T23:20:52.913468Z",
     "shell.execute_reply": "2024-05-07T23:20:52.912219Z",
     "shell.execute_reply.started": "2024-05-07T23:20:52.906336Z"
    },
    "papermill": {
     "duration": 0.022924,
     "end_time": "2022-02-09T18:54:52.200383",
     "exception": false,
     "start_time": "2022-02-09T18:54:52.177459",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    SEED = 3407\n",
    "    USE_TfIdf = True\n",
    "    MAX_FEATURES = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab784339",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T23:20:52.927969Z",
     "iopub.status.busy": "2024-05-07T23:20:52.927575Z",
     "iopub.status.idle": "2024-05-07T23:20:52.940863Z",
     "shell.execute_reply": "2024-05-07T23:20:52.939919Z",
     "shell.execute_reply.started": "2024-05-07T23:20:52.927932Z"
    },
    "papermill": {
     "duration": 0.025406,
     "end_time": "2022-02-09T18:54:52.285484",
     "exception": false,
     "start_time": "2022-02-09T18:54:52.260078",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# arabic\n",
    "def is_arabic(character):\n",
    "    maxchar = max(character)\n",
    "    if u'\\u0627' <= maxchar <= u'\\u064a':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "#korean\n",
    "def is_hangul(value):\n",
    "    if re.search(r'\\p{IsHangul}', value):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#hindi\n",
    "def is_hindi(character):\n",
    "    maxchar = max(character)\n",
    "    if u'\\u0900' <= maxchar <= u'\\u097f':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T23:20:52.943766Z",
     "iopub.status.busy": "2024-05-07T23:20:52.943460Z",
     "iopub.status.idle": "2024-05-07T23:20:52.967442Z",
     "shell.execute_reply": "2024-05-07T23:20:52.966453Z",
     "shell.execute_reply.started": "2024-05-07T23:20:52.943715Z"
    },
    "papermill": {
     "duration": 0.044278,
     "end_time": "2022-02-09T18:54:52.346234",
     "exception": false,
     "start_time": "2022-02-09T18:54:52.301956",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def wrangle_data(train_data, test_data):\n",
    "    test_data['is_test_data'] = 1\n",
    "    train_data['is_test_data'] = 0\n",
    "    data = pd.concat([train_data, test_data]).reset_index(drop=True)\n",
    "\n",
    "    # cat_cols = ['video_id', 'channelId', 'categoryId', 'comments_disabled']\n",
    "    num_cols = ['duration_seconds']\n",
    "    date_cols = ['publishedAt', 'trending_date']\n",
    "    drop_cols = ['id']\n",
    "    \n",
    "    \n",
    "    # ------- Date Time cols ------ \n",
    "    print('Date time cols...')\n",
    "    for col in date_cols:\n",
    "        data[col] = pd.to_datetime(data[col], utc=True)\n",
    "        data.loc[:, col + '_year'] = (data[col].dt.year).astype('category')\n",
    "        data.loc[:, col + '_weekofyear'] = (data[col].dt.isocalendar().week).astype('category')\n",
    "        data.loc[:, col + '_month'] = (data[col].dt.month).astype('category')\n",
    "        data.loc[:, col + '_dayofweek'] = (data[col].dt.dayofweek).astype('category')\n",
    "        data.loc[:, col + '_weekend'] = ((data[col].dt.weekday >=5).astype(int)).astype('category')\n",
    "        drop_cols.append(col)\n",
    "    \n",
    "\n",
    "    data['video_age'] = (data['trending_date'] - data['publishedAt'])\\\n",
    "                        .dt.days.astype('int')\\\n",
    "                        .replace({-1: 0})\n",
    "   \n",
    "    \n",
    "    # ------- Text columns -------\n",
    "    print('Text stuff')\n",
    "    data['text'] = data['channelTitle']\\\n",
    "            + ' ' + data['title'] \\\n",
    "            + ' ' + data['description'].fillna(' ').apply(lambda x: re.sub(r'http\\S+', ' ', x))\\\n",
    "            + ' ' + data['tags'].apply(lambda x: x.replace('|', ' ').replace('[None]', ''))\n",
    "    \n",
    "    data['num_words'] = data['text'].apply(lambda x: len(x.split()))\n",
    "    data['num_characters'] = data['text'].apply(lambda x: len(x))\n",
    "    \n",
    "    drop_cols += ['channelTitle', 'title', 'description', 'tags']\n",
    "    drop_cols += ['thumbnail_link', 'has_thumbnail']\n",
    "    drop_cols += ['view_count', 'likes', 'dislikes', 'comment_count',]\n",
    "\n",
    "    #  ----------- New Features ---------------------------------------\n",
    "    data['Friday_Trending'] = [1 if a == 4 else 0 for a in data.trending_date_dayofweek]\n",
    "    data['Friday_Published'] = [1 if a == 4 else 0 for a in data.publishedAt_dayofweek]\n",
    "    data['Sunday_Published'] = [1 if a == 6 else 0 for a in data.publishedAt_dayofweek]\n",
    "    data['isArabic'] = [is_arabic(a) for a in data.tags]\n",
    "    data['isKorean'] = [is_hangul(a) for a in data.tags]\n",
    "    data['isHindi'] = [is_hindi(a) for a in data.tags]\n",
    "\n",
    "    data.drop(drop_cols, axis=1, inplace=True)\n",
    "    \n",
    "    train_data = data[data['is_test_data'] == 0].reset_index(drop=True)\n",
    "    test_data = data[data['is_test_data'] == 1].reset_index(drop=True)\n",
    "\n",
    "    train_data.drop(['is_test_data'], axis=1, inplace=True)\n",
    "    test_data.drop(['is_test_data'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # scaling\n",
    "    for col in num_cols:\n",
    "        \n",
    "        col_median = train_data[col].median()\n",
    "        train_data[col] = train_data[col].fillna(col_median)\n",
    "        test_data[col] = test_data[col].fillna(col_median)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        train_data[col] = scaler.fit_transform(train_data[[col]])  # Fit and transform on train data\n",
    "        test_data[col] = scaler.transform(test_data[[col]])  # Only transform on test data\n",
    "\n",
    "    # label encoding\n",
    "    cat_cols = train_data.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "    cat_cols.remove('text')\n",
    "    for col in cat_cols:\n",
    "        \n",
    "        train_data[col].fillna('NONE', inplace=True)\n",
    "        known_categories = train_data[col].unique().tolist() + ['Other']\n",
    "    \n",
    "        # Convert train data\n",
    "        le = LabelEncoder()\n",
    "        train_data[col] = le.fit(known_categories).transform(train_data[col])  # Fit on known categories\n",
    "        \n",
    "        # Convert test data, mapping unseen labels to 'Other'\n",
    "        test_data[col] = test_data[col].apply(lambda x: x if x in known_categories else 'Other')\n",
    "        test_data[col] = le.transform(test_data[col])  # Transform using the fitted encoder\n",
    "\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T23:20:52.980048Z",
     "iopub.status.busy": "2024-05-07T23:20:52.979632Z",
     "iopub.status.idle": "2024-05-07T23:20:52.986762Z",
     "shell.execute_reply": "2024-05-07T23:20:52.985777Z",
     "shell.execute_reply.started": "2024-05-07T23:20:52.980018Z"
    },
    "papermill": {
     "duration": 0.023706,
     "end_time": "2022-02-09T18:54:52.428617",
     "exception": false,
     "start_time": "2022-02-09T18:54:52.404911",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def vectorize(train_text, test_text):\n",
    "    vectorizer = TfidfVectorizer(max_features=config.MAX_FEATURES, ngram_range=(1, 2), stop_words='english')\n",
    "    \n",
    "    train_vectors = vectorizer.fit_transform(train_text).toarray() \n",
    "    test_vectors = vectorizer.transform(test_text).toarray()\n",
    "    \n",
    "    return train_vectors, test_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e2cf9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train, test):\n",
    "\n",
    "    train, test = wrangle_data(train, test)\n",
    "    \n",
    "    train_text = train.text.tolist()\n",
    "    test_text = test.text.tolist()\n",
    "\n",
    "    train.drop(['text',], axis=1, inplace=True)\n",
    "    test.drop(['text',], axis=1, inplace=True)\n",
    "\n",
    "    if config.USE_TfIdf:\n",
    "        print('vectorizing...')\n",
    "        \n",
    "        train_vectors, test_vectors = vectorize(train_text, test_text)\n",
    "        \n",
    "        pca = decomposition.PCA(n_components=50, random_state=config.SEED)\n",
    "        print(\"Fitting PCA...\")\n",
    "        pca.fit(train_vectors)\n",
    "        train_projection = pca.transform(train_vectors)\n",
    "        test_projection = pca.transform(test_vectors)\n",
    "        \n",
    "        train_vectors = pd.DataFrame(train_projection)\n",
    "        train_vectors.columns = [f'cat_{i}' for i in range(train_vectors.shape[1])]\n",
    "\n",
    "        test_vectors = pd.DataFrame(test_projection)\n",
    "        test_vectors.columns = [f'cat_{i}' for i in range(test_vectors.shape[1])]\n",
    "        \n",
    "        train = pd.concat([train, train_vectors], axis=1)\n",
    "        test = pd.concat([test, test_vectors], axis=1)\n",
    "        \n",
    "        print('done!')\n",
    "    \n",
    "        drop_cols = ['video_id','comments_disabled']\n",
    "        train.drop(drop_cols, axis=1, inplace=True)\n",
    "        test.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "571e6ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg validation loss: {test_loss:>8f}\")\n",
    "\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(dropout_rate=0.0, use_batchnorm=False, lr=1e-3, num_layers=3, epochs=10):\n",
    "    model = NeuralNetwork(dropout_rate, use_batchnorm, num_layers).to(device)\n",
    "    loss_fn = nn.L1Loss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # early stopping\n",
    "    best_loss = float(\"inf\")\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in validation_dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                total_loss += loss_fn(pred, y).item()\n",
    "        \n",
    "        avg_validation_loss = total_loss / len(validation_dataloader)\n",
    "\n",
    "        # print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        # print(f\"Avg validation loss: {avg_validation_loss:>8f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_validation_loss < best_loss:\n",
    "            best_loss = avg_validation_loss\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    \n",
    "    \n",
    "    return best_loss, round(training_time,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196af51",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91a4ddb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T23:20:53.021296Z",
     "iopub.status.busy": "2024-05-07T23:20:53.020722Z",
     "iopub.status.idle": "2024-05-07T23:20:53.753333Z",
     "shell.execute_reply": "2024-05-07T23:20:53.752487Z",
     "shell.execute_reply.started": "2024-05-07T23:20:53.021269Z"
    },
    "papermill": {
     "duration": 1.821541,
     "end_time": "2022-02-09T18:54:54.353694",
     "exception": false,
     "start_time": "2022-02-09T18:54:52.532153",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('train.parquet')#.sample(500)\n",
    "df = df[df['ratings_disabled'] == 0].reset_index(drop=True)\n",
    "df.drop(['ratings_disabled'], axis=1, inplace=True)\n",
    "df = df.sort_values(by='trending_date').drop_duplicates(subset=['video_id'], keep='first')\n",
    "\n",
    "# splitting data into train and test\n",
    "combined_train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting train further into train and validation\n",
    "train, validation = train_test_split(combined_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T23:20:53.776171Z",
     "iopub.status.busy": "2024-05-07T23:20:53.775778Z",
     "iopub.status.idle": "2024-05-07T23:20:53.839584Z",
     "shell.execute_reply": "2024-05-07T23:20:53.838685Z",
     "shell.execute_reply.started": "2024-05-07T23:20:53.776136Z"
    },
    "papermill": {
     "duration": 6.228952,
     "end_time": "2022-02-09T18:55:00.600004",
     "exception": false,
     "start_time": "2022-02-09T18:54:54.371052",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing combined train and final testing data\n",
      "Date time cols...\n",
      "Text stuff\n",
      "vectorizing...\n",
      "Fitting PCA...\n",
      "done!\n",
      "********************\n",
      "Processing train and validation data\n",
      "Date time cols...\n",
      "Text stuff\n",
      "vectorizing...\n",
      "Fitting PCA...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "print('Processing combined train and final testing data')\n",
    "combined_train, test = preprocess_data(combined_train, test)\n",
    "print('*'*20)\n",
    "print('Processing train and validation data')\n",
    "train, validation = preprocess_data(train, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abec993",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0923d",
   "metadata": {},
   "source": [
    "# **Step 1: Define Your Deep Learning Problem**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea501ab",
   "metadata": {},
   "source": [
    "#### **Problem Statement**\n",
    "\n",
    "YouTube is one of the largest video-sharing platforms, where content creators rely on audience\n",
    "engagement metrics such as likes, views, and comments to measure their success. Deep\n",
    "learning can analyze large-scale YouTube metadata, video features, and historical engagement\n",
    "trends to build a predictive model that estimates the number of likes a video will receive. This\n",
    "can help creators optimize their content strategy, improve audience engagement, and increase\n",
    "monetization opportunities. Coincidentally, this is also a useful tool for brands to analyze and\n",
    "strategically place their YouTube ads.\n",
    "\n",
    "Based on previous EDA and a high level reasoning of what factors could potentially influence user engagement, the following states the task and selected features:\n",
    "\n",
    "- Task\n",
    "    - Regression\n",
    "    \n",
    "- Target\n",
    "    - Like to view ratio\n",
    "    \n",
    "- Features\n",
    "    - Video category\n",
    "    - Duration of video\n",
    "    - Channel name\n",
    "    - Datetime of publishing (and other features engineered from this)\n",
    "    - Age of video at the time of predicting\n",
    "    - length of description\n",
    "    - language of description\n",
    "    - description text (converted into tfidf vectors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "While the dataloader remains same from previous submission, the data has been preprocessed (scaling, label encoding, PCA) to ensure effective training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e7a469",
   "metadata": {},
   "source": [
    "# **Step 2: Train a Neural Network in PyTorch**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce4b9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a480cf0",
   "metadata": {},
   "source": [
    "### Creating Dataset class and defining dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "216f3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        df = df.astype(np.float32) \n",
    "\n",
    "        self.features = df.drop(columns=['target']).values.astype(np.float32)   # Extract features\n",
    "        self.targets = df['target'].values.astype(np.float32)   # Extract target values\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        self.features = torch.tensor(self.features, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(self.targets, dtype=torch.float32).view(-1, 1)  # Reshape for compatibility\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9107676",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_dataset = CustomDataset(combined_train)\n",
    "train_dataset = CustomDataset(train)\n",
    "validation_dataset = CustomDataset(validation)\n",
    "test_dataset = CustomDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3eb8e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_dataloader = DataLoader(combined_train_dataset, batch_size=64)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7899d2d4",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a62a6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(72, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf341b",
   "metadata": {},
   "source": [
    "### Defining loss function, optimizer and other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d568fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669782c",
   "metadata": {},
   "source": [
    "### Training and Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6c7e14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 28.999962  [   64/10808]\n",
      "loss: 0.087681  [ 6464/10808]\n",
      "Avg validation loss: 0.043586\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.040303  [   64/10808]\n",
      "loss: 0.033028  [ 6464/10808]\n",
      "Avg validation loss: 0.041472\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.036227  [   64/10808]\n",
      "loss: 0.032326  [ 6464/10808]\n",
      "Avg validation loss: 0.040080\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.034686  [   64/10808]\n",
      "loss: 0.031779  [ 6464/10808]\n",
      "Avg validation loss: 0.039947\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.034919  [   64/10808]\n",
      "loss: 0.031091  [ 6464/10808]\n",
      "Avg validation loss: 0.039108\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(validation_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c769140",
   "metadata": {},
   "source": [
    "# **Step 2 continued: Try Stuff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08b9d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.0, use_batchnorm=False, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        layers = []\n",
    "        input_size = 72\n",
    "        hidden_size = 512\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(input_size if i == 0 else hidden_size, hidden_size))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "# Experiment settings\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dropout_rates = [0.0, 0.2, 0.5]\n",
    "learning_rates = [1e-3, 1e-2, 1e-4]\n",
    "batchnorm_options = [False, True]\n",
    "no_of_layers = [2, 4, 6]\n",
    "\n",
    "# DataFrames to store results\n",
    "dropout_results = []\n",
    "batchnorm_results = []\n",
    "lr_results = []\n",
    "layer_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d1e00",
   "metadata": {},
   "source": [
    "## Experiment with different dropout rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bcd6bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dropout Rate</th>\n",
       "      <th>Avg Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.037569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.037652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dropout Rate  Avg Validation Loss\n",
       "0           0.0             0.039414\n",
       "1           0.2             0.037569\n",
       "2           0.5             0.037652"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Dropout Experiment\n",
    "for rate in dropout_rates:\n",
    "    avg_loss, _ = train_and_evaluate(dropout_rate=rate)\n",
    "    dropout_results.append({\"Dropout Rate\": rate, \"Avg Validation Loss\": avg_loss})\n",
    "    \n",
    "df_dropout = pd.DataFrame(dropout_results)\n",
    "df_dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff6b71",
   "metadata": {},
   "source": [
    "Even at 0.5 dropout rate, the model loss remains similar as that at 0.0 dropout. This means that there are many redundant neurons that contribute much to final prediction. In this case, using a lighter and sparse model gives similar performance as a fully connected neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36840d0",
   "metadata": {},
   "source": [
    "## Experiment with and without batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "680f9654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BatchNorm</th>\n",
       "      <th>Avg Validation Loss</th>\n",
       "      <th>Training Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.038311</td>\n",
       "      <td>1.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.043508</td>\n",
       "      <td>1.900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BatchNorm  Avg Validation Loss  Training Time\n",
       "0      False             0.038311          1.914\n",
       "1       True             0.043508          1.900"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Batch Normalization Experiment\n",
    "for use_bn in batchnorm_options:\n",
    "    avg_loss, train_time = train_and_evaluate(use_batchnorm=use_bn)\n",
    "    batchnorm_results.append({\"BatchNorm\": use_bn, \"Avg Validation Loss\": avg_loss, \"Training Time\": train_time})\n",
    "\n",
    "df_batchnorm = pd.DataFrame(batchnorm_results)\n",
    "df_batchnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca35ec",
   "metadata": {},
   "source": [
    "There is marginal increase in validation loss when using batch normalization. The training time remains same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48550b",
   "metadata": {},
   "source": [
    "## Experiment with different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be66ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 9\n",
      "Early stopping at epoch 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Avg Validation Loss</th>\n",
       "      <th>Training Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.038203</td>\n",
       "      <td>1.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.067328</td>\n",
       "      <td>1.917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning Rate  Avg Validation Loss  Training Time\n",
       "0         0.0010             0.038203          1.726\n",
       "1         0.0100                  inf          0.557\n",
       "2         0.0001             0.067328          1.917"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Learning Rate Experiment\n",
    "for lr in learning_rates:\n",
    "    avg_loss, train_time = train_and_evaluate(lr=lr)\n",
    "    lr_results.append({\"Learning Rate\": lr, \"Avg Validation Loss\": avg_loss, \"Training Time\": train_time})\n",
    "\n",
    "df_lr = pd.DataFrame(lr_results)\n",
    "df_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e88eb56",
   "metadata": {},
   "source": [
    "Using a larger learning rate speeds up the training time. Using an extremely small learning rate, the model did not converge within the specified epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860f154",
   "metadata": {},
   "source": [
    "## Experiment with number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c36f87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num Layers</th>\n",
       "      <th>Avg Validation Loss</th>\n",
       "      <th>Training Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.038223</td>\n",
       "      <td>1.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.039262</td>\n",
       "      <td>2.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.037656</td>\n",
       "      <td>3.807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Num Layers  Avg Validation Loss  Training Time\n",
       "0           2             0.038223          1.273\n",
       "1           4             0.039262          2.516\n",
       "2           6             0.037656          3.807"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for num_layers in no_of_layers:\n",
    "    avg_loss, train_time = train_and_evaluate(num_layers=num_layers)\n",
    "    layer_results.append({\"Num Layers\": num_layers, \"Avg Validation Loss\": avg_loss, \"Training Time\": train_time})\n",
    "\n",
    "df_layers = pd.DataFrame(layer_results)\n",
    "df_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0433d",
   "metadata": {},
   "source": [
    "Increasing the num layers increases training time while the final validation loss remains same. Maybe for this use case and data, a smaller model is enough to capture the patterns and explain the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985c90a",
   "metadata": {},
   "source": [
    "## **Step 3: Hyperparameter Optimization with Optuna**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdbbe8f",
   "metadata": {},
   "source": [
    "### Define an objective function to be minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "018d347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    dropout_rate = trial.suggest_float(name=\"dropout_rate\", low=0.0, high=0.5, step=0.1)\n",
    "    use_batchnorm = trial.suggest_categorical(\"use_batchnorm\", [True, False])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 7)\n",
    "\n",
    "    model = NeuralNetwork(dropout_rate, use_batchnorm, num_layers).to(device)\n",
    "    loss_fn = nn.L1Loss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluation for pruning\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in validation_dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                total_loss += loss_fn(pred, y).item()\n",
    "        \n",
    "        avg_validation_loss = total_loss / len(validation_dataloader)\n",
    "\n",
    "        # Report intermediate loss and enable pruning\n",
    "        trial.report(avg_validation_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return avg_validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa2407",
   "metadata": {},
   "source": [
    "### Create a study object and optimize the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1927ced9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-14 20:59:02,189] A new study created in memory with name: no-name-300f34ac-8005-4e64-897d-2d8a819fcab1\n",
      "[I 2025-03-14 20:59:03,078] Trial 0 finished with value: 0.03761253644560659 and parameters: {'dropout_rate': 0.30000000000000004, 'use_batchnorm': False, 'learning_rate': 0.0009613188891460426, 'num_layers': 2}. Best is trial 0 with value: 0.03761253644560659.\n",
      "[I 2025-03-14 20:59:05,008] Trial 1 finished with value: 0.03730190255094406 and parameters: {'dropout_rate': 0.0, 'use_batchnorm': False, 'learning_rate': 0.0055831672423104104, 'num_layers': 6}. Best is trial 1 with value: 0.03730190255094406.\n",
      "[I 2025-03-14 20:59:07,692] Trial 2 finished with value: 0.04546715050589207 and parameters: {'dropout_rate': 0.4, 'use_batchnorm': False, 'learning_rate': 0.00030953866577489305, 'num_layers': 6}. Best is trial 1 with value: 0.03730190255094406.\n",
      "[I 2025-03-14 20:59:10,991] Trial 3 finished with value: 0.05744081025206765 and parameters: {'dropout_rate': 0.30000000000000004, 'use_batchnorm': True, 'learning_rate': 0.00013099095369804994, 'num_layers': 6}. Best is trial 1 with value: 0.03730190255094406.\n",
      "[I 2025-03-14 20:59:13,913] Trial 4 finished with value: 0.03764265333843786 and parameters: {'dropout_rate': 0.30000000000000004, 'use_batchnorm': False, 'learning_rate': 0.008542938816907026, 'num_layers': 7}. Best is trial 1 with value: 0.03730190255094406.\n",
      "[I 2025-03-14 20:59:16,421] Trial 5 finished with value: 0.0375106708514829 and parameters: {'dropout_rate': 0.30000000000000004, 'use_batchnorm': False, 'learning_rate': 0.0022965651520421957, 'num_layers': 6}. Best is trial 1 with value: 0.03730190255094406.\n",
      "[I 2025-03-14 20:59:16,671] Trial 6 pruned. \n",
      "[I 2025-03-14 20:59:17,085] Trial 7 pruned. \n",
      "[I 2025-03-14 20:59:17,603] Trial 8 pruned. \n",
      "[I 2025-03-14 20:59:20,183] Trial 9 finished with value: 0.03756921781703483 and parameters: {'dropout_rate': 0.4, 'use_batchnorm': False, 'learning_rate': 0.0034383400501368077, 'num_layers': 6}. Best is trial 1 with value: 0.03730190255094406.\n",
      "[I 2025-03-14 20:59:20,548] Trial 10 pruned. \n",
      "[I 2025-03-14 20:59:23,663] Trial 11 finished with value: 0.03748636779397033 and parameters: {'dropout_rate': 0.1, 'use_batchnorm': False, 'learning_rate': 0.0028781908051451915, 'num_layers': 7}. Best is trial 1 with value: 0.03730190255094406.\n",
      "[I 2025-03-14 20:59:26,837] Trial 12 finished with value: 0.03754484584165174 and parameters: {'dropout_rate': 0.1, 'use_batchnorm': False, 'learning_rate': 0.0029711844620537243, 'num_layers': 7}. Best is trial 1 with value: 0.03730190255094406.\n",
      "[I 2025-03-14 20:59:30,005] Trial 13 finished with value: 0.03765766699473525 and parameters: {'dropout_rate': 0.1, 'use_batchnorm': False, 'learning_rate': 0.0051931139799519166, 'num_layers': 7}. Best is trial 1 with value: 0.03730190255094406.\n",
      "[I 2025-03-14 20:59:30,570] Trial 14 pruned. \n"
     ]
    }
   ],
   "source": [
    "# Run Optuna Study\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=15)\n",
    "\n",
    "df_trials = pd.DataFrame([\n",
    "    {**trial.params, \"Validation Loss\": trial.value, \"Pruned\": trial.state == optuna.trial.TrialState.PRUNED}\n",
    "    for trial in study.trials\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b49f7e8",
   "metadata": {},
   "source": [
    "### Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a930fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Trials\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>use_batchnorm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pruned</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trial</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>2</td>\n",
       "      <td>0.037613</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>6</td>\n",
       "      <td>0.037302</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>6</td>\n",
       "      <td>0.045467</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>6</td>\n",
       "      <td>0.057441</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.008543</td>\n",
       "      <td>7</td>\n",
       "      <td>0.037643</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>6</td>\n",
       "      <td>0.037511</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>3</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>5</td>\n",
       "      <td>0.079076</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>2</td>\n",
       "      <td>0.044388</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>6</td>\n",
       "      <td>0.037569</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>4</td>\n",
       "      <td>0.381602</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>7</td>\n",
       "      <td>0.037486</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>7</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.005193</td>\n",
       "      <td>7</td>\n",
       "      <td>0.037658</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>5</td>\n",
       "      <td>0.085951</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dropout_rate  use_batchnorm  learning_rate  num_layers  \\\n",
       "Trial                                                           \n",
       "0               0.3          False       0.000961           2   \n",
       "1               0.0          False       0.005583           6   \n",
       "2               0.4          False       0.000310           6   \n",
       "3               0.3           True       0.000131           6   \n",
       "4               0.3          False       0.008543           7   \n",
       "5               0.3          False       0.002297           6   \n",
       "6               0.3          False       0.000810           3   \n",
       "7               0.4          False       0.000783           5   \n",
       "8               0.0           True       0.000120           2   \n",
       "9               0.4          False       0.003438           6   \n",
       "10              0.0           True       0.009967           4   \n",
       "11              0.1          False       0.002878           7   \n",
       "12              0.1          False       0.002971           7   \n",
       "13              0.1          False       0.005193           7   \n",
       "14              0.1           True       0.002222           5   \n",
       "\n",
       "       Validation Loss  Pruned  \n",
       "Trial                           \n",
       "0             0.037613   False  \n",
       "1             0.037302   False  \n",
       "2             0.045467   False  \n",
       "3             0.057441   False  \n",
       "4             0.037643   False  \n",
       "5             0.037511   False  \n",
       "6             0.056433    True  \n",
       "7             0.079076    True  \n",
       "8             0.044388    True  \n",
       "9             0.037569   False  \n",
       "10            0.381602    True  \n",
       "11            0.037486   False  \n",
       "12            0.037545   False  \n",
       "13            0.037658   False  \n",
       "14            0.085951    True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Trial:\n",
      "{'dropout_rate': 0.0, 'use_batchnorm': False, 'learning_rate': 0.0055831672423104104, 'num_layers': 6}\n"
     ]
    }
   ],
   "source": [
    "# Print the best hyperparameter configuration\n",
    "df_trials.index.names = ['Trial']\n",
    "print('All Trials')\n",
    "display(df_trials)\n",
    "print(\"\\nBest Trial:\")\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf7bae",
   "metadata": {},
   "source": [
    "## **Step 3 continued: Insights**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb67e89",
   "metadata": {},
   "source": [
    "Using Optuna, I was able to run 14 trials within a short span. Some of the non effective trials were also pruned, thus saving more experimentation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0152d",
   "metadata": {},
   "source": [
    "## **Step 4: Final Training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cfeb889a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters: {'dropout_rate': 0.0, 'use_batchnorm': False, 'learning_rate': 0.0055831672423104104, 'num_layers': 6}\n",
      "\n",
      "Training final model...\n",
      "\n",
      "Final Model Test Loss: 0.039236\n"
     ]
    }
   ],
   "source": [
    "# Extract the best parameters from Optuna\n",
    "best_params = study.best_trial.params\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "\n",
    "# Define and train the final model with the best hyperparameters\n",
    "final_model = NeuralNetwork(\n",
    "    dropout_rate=best_params[\"dropout_rate\"],\n",
    "    use_batchnorm=best_params[\"use_batchnorm\"],\n",
    "    num_layers=best_params[\"num_layers\"]\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = optim.SGD(final_model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "\n",
    "# Training final model on the combined dataset\n",
    "epochs = 10\n",
    "print(\"\\nTraining final model...\")\n",
    "for epoch in range(epochs):\n",
    "    final_model.train()\n",
    "    for batch, (X, y) in enumerate(combined_train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = final_model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "final_model.eval()\n",
    "total_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = final_model(X)\n",
    "        total_test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "\n",
    "# Print the final test metric\n",
    "print(f\"\\nFinal Model Test Loss: {avg_test_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade8368",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60611a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2514.337492,
   "end_time": "2022-02-09T19:36:25.333348",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-09T18:54:30.995856",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "089aa6460ce34c508730028668a0869b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fe4a93f15715449c8c0b2dec2927ec36",
        "IPY_MODEL_f0d3c51073c345739245dee44ee1e7d0",
        "IPY_MODEL_64de8b4a32964ca3a562273e0cada2de"
       ],
       "layout": "IPY_MODEL_1b7d32473b944fcea006bf4218e54acf"
      }
     },
     "1b544e9c45ad46978f244711723e320a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1b7d32473b944fcea006bf4218e54acf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a5e5e62e36e4ab1ab498ca75f167f31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "64de8b4a32964ca3a562273e0cada2de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b9208384593b4564a4540d8a9142bdb5",
       "placeholder": "​",
       "style": "IPY_MODEL_3a5e5e62e36e4ab1ab498ca75f167f31",
       "value": " 10/10 [39:31&lt;00:00, 229.53s/it]"
      }
     },
     "9d7fb602c019482395dc0bfcafd7ec61": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9208384593b4564a4540d8a9142bdb5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d1f2158464c9415ba2f571dd2ed63345": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d45be424d16241e38393233510ceb1b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f0d3c51073c345739245dee44ee1e7d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9d7fb602c019482395dc0bfcafd7ec61",
       "max": 10,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d45be424d16241e38393233510ceb1b6",
       "value": 10
      }
     },
     "fe4a93f15715449c8c0b2dec2927ec36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d1f2158464c9415ba2f571dd2ed63345",
       "placeholder": "​",
       "style": "IPY_MODEL_1b544e9c45ad46978f244711723e320a",
       "value": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
